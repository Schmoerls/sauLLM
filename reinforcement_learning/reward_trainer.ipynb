{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8faff98f-ffa4-434c-9fc4-03c1bd2f79fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/trl/blob/main/examples/scripts/reward_modeling.py\n",
    "# https://huggingface.co/docs/trl/v0.7.10/en/reward_trainer\n",
    "\n",
    "# https://www.youtube.com/watch?v=_2qiJXUc798\n",
    "# https://colab.research.google.com/github/githubpradeep/notebooks/blob/main/Reward_Model_for_RLHF_%2B_trl.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8dac717-e22f-4e73-bc4f-34dc98dcc991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from peft import PeftModel\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d00ae44-147c-4d1f-a73f-481d712ab9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.9.post4: Fast Mistral patching. Transformers = 4.45.1.\n",
      "   \\\\   /|    GPU: Quadro RTX 6000. Max memory: 23.462 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.1. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    }
   ],
   "source": [
    "base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/mistral-7b-v0.3\", \n",
    "    local_files_only = True, \n",
    "    max_seq_length = 2048, # Choose any! We auto support RoPE Scaling internally!\n",
    "    dtype = None, # None for auto detection. Float16 \"or Tesla T4, V100, Bfloat16 for Ampere+\n",
    "    load_in_4bit = True, # Use 4bit quantization to reduce memory usage. Can be False.\n",
    ")\n",
    "\n",
    "peft_model_gpt = PeftModel.from_pretrained(base_model, \"../fullRun/lorasConfig1/checkpoint-862/\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6789d7a8-cc22-4a27-beed-0ae8369c6628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load data from a folder\n",
    "def load_data_from_folder(folder):\n",
    "    data = []\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith('.json'):  # Assuming JSON files\n",
    "            file_path = os.path.join(folder, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                try:\n",
    "                    file_data = json.load(f)\n",
    "                    data.extend(file_data)  # Add all entries to the list\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Error decoding JSON in file {filename}\")\n",
    "    return data\n",
    "\n",
    "# Create the dataset for the reward trainer\n",
    "def create_reward_trainer_dataset(good_data_folder, bad_data_folder, output_csv_file):\n",
    "    # Load good data from folder\n",
    "    good_data = load_data_from_folder(good_data_folder)\n",
    "    \n",
    "    # Load bad data from folder\n",
    "    bad_data = load_data_from_folder(bad_data_folder)\n",
    "    \n",
    "    # Prepare the data for the reward trainer\n",
    "    chosen_data = [f\"Input: {entry['input']} | Output: {entry['output']}\" for entry in good_data]\n",
    "    rejected_data = [f\"Input: {entry['input']} | Output: {entry['output']}\" for entry in bad_data]\n",
    "    \n",
    "    # Ensure both columns have the same length by padding with empty strings if necessary\n",
    "    max_len = max(len(chosen_data), len(rejected_data))\n",
    "    chosen_data += [''] * (max_len - len(chosen_data))\n",
    "    rejected_data += [''] * (max_len - len(rejected_data))\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'chosen': chosen_data,\n",
    "        'rejected': rejected_data\n",
    "    })\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(output_csv_file, index=False, encoding='utf-8')\n",
    "    print(f\"Dataset saved to {output_csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d771a81c-16d5-4d0c-9fa6-819a2ed21ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_data_folder = \"./good_data/\"\n",
    "bad_data_folder = \"./bad_data/\"\n",
    "output_csv_file = \"reward_trainer_dataset.csv\"\n",
    "\n",
    "create_reward_trainer_dataset(good_data_folder, bad_data_folder, output_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a1d9d39-8850-47c6-9ef2-dbc9aa9afc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['chosen', 'rejected'],\n",
      "    num_rows: 22040\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Need a train_dataset with columns \"chosen\" and \"rejected\"\n",
    "train_dataset = pd.read_csv('reward_trainer_dataset.csv', encoding='utf-8')\n",
    "train_dataset = Dataset.from_pandas(train_dataset)\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1f7a8a2-c65b-447f-b78b-2758e7d40468",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88563050-6bf7-460e-8092-3d131f2e40b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changed version from:\n",
    "# https://colab.research.google.com/github/githubpradeep/notebooks/blob/main/Reward_Model_for_RLHF_%2B_trl.ipynb\n",
    "# preprocesses data into specific form for reward_trainer\n",
    "def preprocess_function(examples):\n",
    "    new_examples = {\n",
    "        \"input_ids_chosen\": [],\n",
    "        \"attention_mask_chosen\": [],\n",
    "        \"input_ids_rejected\": [],\n",
    "        \"attention_mask_rejected\": [],\n",
    "    }\n",
    "    for chosen, rejected in zip(examples[\"chosen\"], examples[\"rejected\"]):\n",
    "        if isinstance(chosen, str) and isinstance(rejected, str):\n",
    "            tokenized_j = tokenizer(chosen, truncation=True, max_length=512, padding=\"max_length\")\n",
    "            tokenized_k = tokenizer(rejected, truncation=True, max_length=512, padding=\"max_length\")\n",
    "\n",
    "            # Only add if tokens exist\n",
    "            if tokenized_j[\"input_ids\"] and tokenized_k[\"input_ids\"]:\n",
    "                new_examples[\"input_ids_chosen\"].append(tokenized_j[\"input_ids\"])\n",
    "                new_examples[\"attention_mask_chosen\"].append(tokenized_j[\"attention_mask\"])\n",
    "                new_examples[\"input_ids_rejected\"].append(tokenized_k[\"input_ids\"])\n",
    "                new_examples[\"attention_mask_rejected\"].append(tokenized_k[\"attention_mask\"])\n",
    "        else:\n",
    "            # Handle missing cases by adding zeros instead of empty lists\n",
    "            new_examples[\"input_ids_chosen\"].append([0] * 512)\n",
    "            new_examples[\"attention_mask_chosen\"].append([0] * 512)\n",
    "            new_examples[\"input_ids_rejected\"].append([0] * 512)\n",
    "            new_examples[\"attention_mask_rejected\"].append([0] * 512)\n",
    "\n",
    "    return new_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "528622af-e041-4ae5-9778-c86cfacc87b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['chosen', 'rejected'],\n",
      "    num_rows: 22040\n",
      "})\n",
      "Input: Was passiert mit dem Rückzahlungsanspruch für einen Pauschvergütungsvorschuss, wenn die Verjährung des Vergütungsanspruchs eingetreten ist? | Output: Was passiert mit dem Rückzahlungsanspruch für einen Pauschvergütungsvorschuss, wenn die Verjährung des Vergütungsanspruchs eingetreten ist?\n",
      "\n",
      "Welche Ansprüche GmbH xxZ xy GbRH uG GmBv 201995 AnsprucheGmb H xNrG 02 AnstaltreuBundesG mbH  xY G bR H  u G m B v 4 AnstaltreuG m H y G aR U G mb H a4 TreuHandW xC HandW aG Handw xZ Hand W a G  aZ  Treug Hand w aCh Hand D Hand P Hand p Hand q Hand r Hand s Hand u Hand t Hand ü Hand v Hand y Hand z Hand ä Hand Ö Hand Ä Hand ö Hand Ø Hand ó HandÖ Hand × HandÜ Hand× Hand÷ Hand± Hand�� Hand¼ Hand½ Hand¾ Handß Hand¶ Hand· Hand§ Handà Handå Handù Handä Handø Handö Handõ Handō Handü Handüd Handý Handy Handž Handz Hand| Handwe Handwi Handwo Handx Handxy Handxx Handÿ Hand xzHand xə Handź Handż Hand Ž Hand ž Hand Ż Hand ż HandŽ HandŻ HandÅ Hand Å HandÄ HandÃ Handã Handñ Handń Handğ Handļ Handł Handś Handą Handů Handç Handę Handš HandŠ Handœ Handð Handδ Handė Handđ HandĖ Handķ Handľ Handść Handř Handží Hand HandŹ Handć HandČ Handč Hand Č Hand č Hand d Hand dz Hand dv Hand dw Hand dx Hand dy Hand dys Hand dt Hand l Hand lu Hand lo Hand wo Hand wp Hand wx Hand wy Hand zm Hand zn Hand zw Hand za Hand az Hand b Hand bc Hand cb Hand db Hand dc Hand dl Hand el Hand em Hand en Hand er Hand es Hand è Hand é Hand È Hand É Hand E Hand e Handë Handê Handël Handül Handün Handū Handũ Handų Handüss Handös Handöt Handöß HandŅ Handṗ Handḍ HandǗ  Handṇ Handży Hand з Handж Hand Ж Hand ж HandЖ Hand З Handз Handа Hand б Hand а Handв Hand у Hand ú Handю Hand я Hand Я HandЯ Handя Handја Handђ Handћ Handď Handџ Handд Hand д Hand е Hand э Handё Handө Handь Handъ Handч Hand ч Hand Ч Hand ш Hand Ш Handш HandШ HandЩ HandЪ HandЫ HandЬ HandЇ HandЙ HandЈ Handј Handљ Handл Handм Hand м Handн Handо Handп Handπ Handρ Handσ Handς Handѕ Handzs Handzt Handdt Handdw Handdx Handdy Handdz Handdv Handzw Handwt Handwb Handwx Handwy Handzm Handzn Handzon Handzo Handza Handb Handbc Handcb Handdb Handdc Handdl Handel Handem Handen Hander Handes Handès Handэ Handě Handдь Handђе Handġ Handќ Hand╝ Handы Handзы Handц Handцо Handрез Handжи Handзи Handзь Handза Handда Handб Handa Handv Handu Handuv Handuw Handvy Handwm Handwn Handxd Handxe Handzy Handzz Handyt Handvt Handwd Handwq Handxb Handxc Handzd Handze Handzte Handft Handtf Handtg HandugHand ug Handuj Handuk Handul Handum Handun Handur Handus Handuß Handу Handű Hand� Handү Handї Handи Handі Handй Handzig Handζ Handжа Handʐ Handже Handжде Handže Handжда Handнд Handне Handни Handно Handпо Handпу Handпи Handри Handры Handрь Handр Handс Handсь Handть Handсть Handт Handту Handцу Handца Handзя Handбе Handве Handга Handге Handґ Handе Handє Handје Handју Handju Handжу Handзда Handзд Handзе Handте Handтво Hand tf HandUG HandUgHand UgB HandUB Handub Handvb Handvv Handww Handwh Handwc Handws Handxt Handqt HandXT HandXt Hand XtHand XT Hand Yt \n",
      "Was ist die Schlüssigkeit der Feststellung in Bezug II Ziv 109/96 des Beschlusses in dem vorliegenden Urteils des § 39 Absatz 5 des Gesetzes in den vorhergestellten Urteil des§ 60 Absatzen Urteleils in der vorjahrestellen Urteeils der §§136 Absätzen  Urätel in vorhajrestelen Urtéilsder §16 Ä zen Uratél in Vorhajařtelen Urtélsder Vorjajřtélen Urjazélén in Vorgjázélën Urzázéłén In Vörgjážélö in Worgžáżélø in Woržažéľ in Wořgžéléł in Waržežèleł в Warżéлë в Wöržěлє в Worżěłé в Воръжѣлъ в Воржěлё в Vorжėлё в Vooržėlё in Foržělœ in Forežëlø в Forzělø In Forżělø Endschließ in Endschel in endschlö Endžlœ Endżlë in Éndschлœ в Éндžлѣ в Энджлě в Endʒлэ в Ендʒлё in Энџлė в Энжлё Endжле вЭндзл�ě in Ӽжlé вӽжлее вҘжзлё vҙжље вЖжжё vЖžле vŽжžлёв ЖжжелЁ в Жорʒléв Žорżлёv Žorżlé in Žörʒłёв Zоргжèle в Zоръʒлев Зорзжёлв zОръгʒlєв зОргжелёw Zorgʒle в zOrgʒlle в Zagžléw Zagżлèlew Zażглѐw Azagʒллеw Azażłèleв Azorgжllew Arggʒlléw Argʒľé in Argžlle Arżgłëw Angżlle Angžłłęw Anžglle Anżżľę in Anzžľěw Andżлле в Andzʒℓëв Andžžℓé w Andźżℓę в Anđżḷе в Анжʒḷэ in Анџжлле Анзґлęв Анђжℓэв Ąндżле in Андзьлië в Анђгжде вАндгžллев Ангжждев Angdżelle в Angźждèle in Angz żdelleв Anźždèle w An żżdëłe в Ańżдѣłе w Añżdzelle w Anyżdziełè w Ńżďéllë w Аньжdzieлè в Anyжђлĕ в Áньżḍлê в Àньžдзіллв Àндźдлї вÀндъжḍℓвÀндждзℓvÀнгџдзел вÁнгзджевАнгджѐлvÁнґдзьдждел vÁндґжђелзвÁдґзждеłвAндѧжздждеlvАдѩжďелzвӧджздзdlvАджждьезlvӦджждезльвЭджђжделевЭдżжделёлЭџђздзеłvЭжджdzieвÉжџздзівЁжждаздзилЄжждуздзылЈждаздzyлЌждарздзялСжdarздялЗждатздъЗорджаздЪжданздЫждыздЗѓдардждызДжїдардздДѱждерздЖѡдарждизЖздаржизЗҡарждізҟжардждиЗъарджидЗыждииЗьаржилЬжжильЗюждялЪžарџдилҥждияздҧžдарџилӥџдыжҵждинздӡжыздѥджызӢжзыздХдзиздОжздазЫдзыдЫзОѢзджиздЁздыздалЖьдарживазджДыздальЖҗдарживадžДьзджалжЖъдаржадЖыждалzhЖадзядzhŽаздžядZжаздажZхарђазZдъаждажЗхъдarzжdZhаdzажdzZhdZžаџаż\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print(train_dataset[501][\"rejected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95350ef9-6568-4ee8-bee7-d6fec8c4f502",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bdde3d88fec40b887a57ee9c8d20143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22040 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8b959fbe2c84a278e911067203f5fc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/22040 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['chosen', 'rejected', 'input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected'],\n",
      "    num_rows: 22040\n",
      "})\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    ")\n",
    "train_dataset = train_dataset.filter(\n",
    "    lambda x: len(x[\"input_ids_chosen\"]) <= 512\n",
    "    and len(x[\"input_ids_rejected\"]) <= 512\n",
    ")\n",
    "\n",
    "print(train_dataset)\n",
    "print(type(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9a3c8be-ff88-43d9-839a-0810c56c02d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98740d14-e03f-4f62-9c2a-2a4e1ddd858f",
   "metadata": {},
   "source": [
    "# RewardTrainer benutzen, um unser Modell zu verbessern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b92913-d537-45ec-b461-dba7ba41e827",
   "metadata": {},
   "source": [
    "## Model GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee19c1b-48ec-43cc-8769-22a85af39f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have to transform the parameters in our lora adapters, so they can be changed by the reward trainer\n",
    "# the code enables gradient tracking for all parameters\n",
    "# but first they have to be floating-point type (like torch.float32), which is required for gradients to be computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf9a7315-aacd-472b-8b93-4115570b5af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in peft_model_gpt.parameters():\n",
    "    param.data = param.data.float()  # Convert to float if not already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "817108f8-ea98-4a71-a7c7-98afb6bb005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in peft_model_gpt.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45606105-743d-45f1-812e-6a31b097795c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in peft_model_gpt.parameters():\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "323d3ec4-fcea-4f9e-a27d-7c5f173655d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "\n",
    "# set the wandb project where this run will be logged\n",
    "os.environ[\"WANDB_PROJECT\"]=\"sauLLM\"\n",
    "\n",
    "# save your trained model checkpoint to wandb\n",
    "os.environ[\"WANDB_LOG_MODEL\"]=\"checkpoint\"\n",
    "\n",
    "# turn off watch to log faster\n",
    "os.environ[\"WANDB_WATCH\"]=\"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27480af9-d3a4-42f9-a600-9957501a61a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 22,040 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 2\n",
      "\\        /    Total batch size = 8 | Total steps = 100\n",
      " \"-____-\"     Number of trainable parameters = 3,800,305,664\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mm-schmerle\u001b[0m (\u001b[33mm-schmerle-universit-t-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ps2024/sauLLM/reinforcement_learning/wandb/run-20241102_145504-onr6n6ef</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/m-schmerle-universit-t-/sauLLM/runs/onr6n6ef' target=\"_blank\">./model_gpt/train_logs</a></strong> to <a href='https://wandb.ai/m-schmerle-universit-t-/sauLLM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/m-schmerle-universit-t-/sauLLM' target=\"_blank\">https://wandb.ai/m-schmerle-universit-t-/sauLLM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/m-schmerle-universit-t-/sauLLM/runs/onr6n6ef' target=\"_blank\">https://wandb.ai/m-schmerle-universit-t-/sauLLM/runs/onr6n6ef</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 1:22:25, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./model_gpt/train_logs/checkpoint-20)... Done. 4.5s\n",
      "/home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./model_gpt/train_logs/checkpoint-40)... Done. 4.6s\n",
      "/home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./model_gpt/train_logs/checkpoint-60)... Done. 4.6s\n",
      "/home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./model_gpt/train_logs/checkpoint-80)... Done. 4.6s\n",
      "/home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./model_gpt/train_logs/checkpoint-100)... Done. 4.6s\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from peft import LoraConfig\n",
    "from trl import RewardTrainer, RewardConfig\n",
    "\n",
    "# Step 1: Set up reward and LoRA configurations\n",
    "reward_config = RewardConfig(\n",
    "    output_dir=\"./model_gpt/train_logs\",\n",
    "    max_steps=100,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1.41e-5,\n",
    "    optim=\"adamw_torch\",\n",
    "    save_steps=20,  # Save checkpoints during training to monitor, but we'll keep only the final model\n",
    "    logging_steps=20,\n",
    "    report_to=\"wandb\",\n",
    "    remove_unused_columns=False,\n",
    "    max_length=512,\n",
    "    gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    inference_mode=False, # has to be false or else the reward trainer can't adjust the parameters\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0\n",
    ")\n",
    "\n",
    "# Step 2: Initialize RewardTrainer with your LoRA-enhanced model\n",
    "reward_trainer = RewardTrainer(\n",
    "    model=peft_model_gpt,  \n",
    "    tokenizer=tokenizer,\n",
    "    args=reward_config,\n",
    "    train_dataset=train_dataset,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "# Step 3: Train the model with reward feedback\n",
    "reward_trainer.train()\n",
    "\n",
    "# Step 4: Save only the improved model\n",
    "reward_trainer.model.save_pretrained(\"./improved_peft_model_gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb4a43b5-985b-4c0a-9c51-7154ba257242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.9.post4: Fast Mistral patching. Transformers = 4.45.1.\n",
      "   \\\\   /|    GPU: Quadro RTX 6000. Max memory: 23.462 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.1. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Model and Tokenizer Loading\n",
    "base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/mistral-7b-v0.3\", \n",
    "    local_files_only=True, \n",
    "    max_seq_length=2048, \n",
    "    dtype=None, \n",
    "    load_in_4bit=True \n",
    ")\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(base_model, \"../reinforcement_learning/improved_peft_model_gpt/\") \n",
    "\n",
    "FastLanguageModel.for_inference(peft_model)\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd9c5764-fe8f-4c4d-aa25-fd2d26a5f07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Was ist Mord?\"\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(\n",
    "    input_text,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=tokenizer.model_max_length\n",
    ").to(device)\n",
    "\n",
    "# Generate output\n",
    "with torch.no_grad():\n",
    "    outputs = peft_model.generate(\n",
    "        **inputs,\n",
    "        max_length=2048,\n",
    "        repetition_penalty=1.2,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a4496d6-bc50-4f36-8be3-5dfbfbd3aedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Was ist Mord?\n"
     ]
    }
   ],
   "source": [
    "# Decode the output\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1549604-9459-4db5-ade3-826ad1aea866",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
