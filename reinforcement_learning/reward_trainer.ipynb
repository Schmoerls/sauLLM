{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8faff98f-ffa4-434c-9fc4-03c1bd2f79fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/trl/blob/main/examples/scripts/reward_modeling.py\n",
    "# https://huggingface.co/docs/trl/v0.7.10/en/reward_trainer\n",
    "\n",
    "# https://www.youtube.com/watch?v=_2qiJXUc798\n",
    "# https://colab.research.google.com/github/githubpradeep/notebooks/blob/main/Reward_Model_for_RLHF_%2B_trl.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8dac717-e22f-4e73-bc4f-34dc98dcc991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from peft import PeftModel\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d00ae44-147c-4d1f-a73f-481d712ab9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.9.post4: Fast Mistral patching. Transformers = 4.45.1.\n",
      "   \\\\   /|    GPU: Quadro RTX 6000. Max memory: 23.462 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.1. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    }
   ],
   "source": [
    "base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/mistral-7b-v0.3\", \n",
    "    local_files_only = True, \n",
    "    max_seq_length = 2048, # Choose any! We auto support RoPE Scaling internally!\n",
    "    dtype = None, # None for auto detection. Float16 \"or Tesla T4, V100, Bfloat16 for Ampere+\n",
    "    load_in_4bit = True, # Use 4bit quantization to reduce memory usage. Can be False.\n",
    ")\n",
    "\n",
    "peft_model_gpt = PeftModel.from_pretrained(base_model, \"../fullRun/lorasConfig1/checkpoint-862/\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6789d7a8-cc22-4a27-beed-0ae8369c6628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load data from a folder\n",
    "def load_data_from_folder(folder):\n",
    "    data = []\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith('.json'):  # Assuming JSON files\n",
    "            file_path = os.path.join(folder, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                try:\n",
    "                    file_data = json.load(f)\n",
    "                    data.extend(file_data)  # Add all entries to the list\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Error decoding JSON in file {filename}\")\n",
    "    return data\n",
    "\n",
    "# Create the dataset for the reward trainer\n",
    "def create_reward_trainer_dataset(good_data_folder, bad_data_folder, output_csv_file):\n",
    "    # Load good data from folder\n",
    "    good_data = load_data_from_folder(good_data_folder)\n",
    "    \n",
    "    # Load bad data from folder\n",
    "    bad_data = load_data_from_folder(bad_data_folder)\n",
    "    \n",
    "    # Prepare the data for the reward trainer\n",
    "    chosen_data = [f\"Input: {entry['input']} | Output: {entry['output']}\" for entry in good_data]\n",
    "    rejected_data = [f\"Input: {entry['input']} | Output: {entry['output']}\" for entry in bad_data]\n",
    "    \n",
    "    # Ensure both columns have the same length by padding with empty strings if necessary\n",
    "    max_len = max(len(chosen_data), len(rejected_data))\n",
    "    chosen_data += [''] * (max_len - len(chosen_data))\n",
    "    rejected_data += [''] * (max_len - len(rejected_data))\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'chosen': chosen_data,\n",
    "        'rejected': rejected_data\n",
    "    })\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(output_csv_file, index=False, encoding='utf-8')\n",
    "    print(f\"Dataset saved to {output_csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d771a81c-16d5-4d0c-9fa6-819a2ed21ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_data_folder = \"./good_data/\"\n",
    "bad_data_folder = \"./bad_data/\"\n",
    "output_csv_file = \"reward_trainer_dataset.csv\"\n",
    "\n",
    "create_reward_trainer_dataset(good_data_folder, bad_data_folder, output_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a1d9d39-8850-47c6-9ef2-dbc9aa9afc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['chosen', 'rejected'],\n",
      "    num_rows: 22040\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Need a train_dataset with columns \"chosen\" and \"rejected\"\n",
    "train_dataset = pd.read_csv('reward_trainer_dataset.csv', encoding='utf-8')\n",
    "train_dataset = Dataset.from_pandas(train_dataset)\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1f7a8a2-c65b-447f-b78b-2758e7d40468",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88563050-6bf7-460e-8092-3d131f2e40b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changed version from:\n",
    "# https://colab.research.google.com/github/githubpradeep/notebooks/blob/main/Reward_Model_for_RLHF_%2B_trl.ipynb\n",
    "# preprocesses data into specific form for reward_trainer\n",
    "def preprocess_function(examples):\n",
    "    new_examples = {\n",
    "        \"input_ids_chosen\": [],\n",
    "        \"attention_mask_chosen\": [],\n",
    "        \"input_ids_rejected\": [],\n",
    "        \"attention_mask_rejected\": [],\n",
    "    }\n",
    "    for chosen, rejected in zip(examples[\"chosen\"], examples[\"rejected\"]):\n",
    "        if isinstance(chosen, str) and isinstance(rejected, str):\n",
    "            tokenized_j = tokenizer(chosen, truncation=True, max_length=512, padding=\"max_length\")\n",
    "            tokenized_k = tokenizer(rejected, truncation=True, max_length=512, padding=\"max_length\")\n",
    "\n",
    "            # Only add if tokens exist\n",
    "            if tokenized_j[\"input_ids\"] and tokenized_k[\"input_ids\"]:\n",
    "                new_examples[\"input_ids_chosen\"].append(tokenized_j[\"input_ids\"])\n",
    "                new_examples[\"attention_mask_chosen\"].append(tokenized_j[\"attention_mask\"])\n",
    "                new_examples[\"input_ids_rejected\"].append(tokenized_k[\"input_ids\"])\n",
    "                new_examples[\"attention_mask_rejected\"].append(tokenized_k[\"attention_mask\"])\n",
    "        else:\n",
    "            # Handle missing cases by adding zeros instead of empty lists\n",
    "            new_examples[\"input_ids_chosen\"].append([0] * 512)\n",
    "            new_examples[\"attention_mask_chosen\"].append([0] * 512)\n",
    "            new_examples[\"input_ids_rejected\"].append([0] * 512)\n",
    "            new_examples[\"attention_mask_rejected\"].append([0] * 512)\n",
    "\n",
    "    return new_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "528622af-e041-4ae5-9778-c86cfacc87b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['chosen', 'rejected'],\n",
      "    num_rows: 22040\n",
      "})\n",
      "Input: Was passiert mit dem R√ºckzahlungsanspruch f√ºr einen Pauschverg√ºtungsvorschuss, wenn die Verj√§hrung des Verg√ºtungsanspruchs eingetreten ist? | Output: Was passiert mit dem R√ºckzahlungsanspruch f√ºr einen Pauschverg√ºtungsvorschuss, wenn die Verj√§hrung des Verg√ºtungsanspruchs eingetreten ist?\n",
      "\n",
      "Welche Anspr√ºche GmbH xxZ xy GbRH uG GmBv 201995 AnsprucheGmb H xNrG 02 AnstaltreuBundesG mbH  xY G bR H  u G m B v 4 AnstaltreuG m H y G aR U G mb H a4 TreuHandW xC HandW aG Handw xZ Hand W a G  aZ  Treug Hand w aCh Hand D Hand P Hand p Hand q Hand r Hand s Hand u Hand t Hand √º Hand v Hand y Hand z Hand √§ Hand √ñ Hand √Ñ Hand √∂ Hand √ò Hand √≥ Hand√ñ Hand √ó Hand√ú Hand√ó Hand√∑ Hand¬± HandÔøΩÔøΩ Hand¬º Hand¬Ω Hand¬æ Hand√ü Hand¬∂ Hand¬∑ Hand¬ß Hand√† Hand√• Hand√π Hand√§ Hand√∏ Hand√∂ Hand√µ Hand≈ç Hand√º Hand√ºd Hand√Ω Handy Hand≈æ Handz Hand| Handwe Handwi Handwo Handx Handxy Handxx Hand√ø Hand xzHand x…ô Hand≈∫ Hand≈º Hand ≈Ω Hand ≈æ Hand ≈ª Hand ≈º Hand≈Ω Hand≈ª Hand√Ö Hand √Ö Hand√Ñ Hand√É Hand√£ Hand√± Hand≈Ñ Handƒü Handƒº Hand≈Ç Hand≈õ HandƒÖ Hand≈Ø Hand√ß Handƒô Hand≈° Hand≈† Hand≈ì Hand√∞ HandŒ¥ Handƒó Handƒë Handƒñ Handƒ∑ Handƒæ Hand≈õƒá Hand≈ô Hand≈æ√≠ Hand¬û Hand≈π Handƒá Handƒå Handƒç Hand ƒå Hand ƒç Hand d Hand dz Hand dv Hand dw Hand dx Hand dy Hand dys Hand dt Hand l Hand lu Hand lo Hand wo Hand wp Hand wx Hand wy Hand zm Hand zn Hand zw Hand za Hand az Hand b Hand bc Hand cb Hand db Hand dc Hand dl Hand el Hand em Hand en Hand er Hand es Hand √® Hand √© Hand √à Hand √â Hand E Hand e Hand√´ Hand√™ Hand√´l Hand√ºl Hand√ºn Hand≈´ Hand≈© Hand≈≥ Hand√ºss Hand√∂s Hand√∂t Hand√∂√ü Hand≈Ö Hand·πó Hand·∏ç Hand«ó  Hand·πá Hand≈ºy Hand –∑ Hand–∂ Hand –ñ Hand –∂ Hand–ñ Hand –ó Hand–∑ Hand–∞ Hand –± Hand –∞ Hand–≤ Hand —É Hand √∫ Hand—é Hand —è Hand –Ø Hand–Ø Hand—è Hand—ò–∞ Hand—í Hand—õ Handƒè Hand—ü Hand–¥ Hand –¥ Hand –µ Hand —ç Hand—ë Hand”© Hand—å Hand—ä Hand—á Hand —á Hand –ß Hand —à Hand –® Hand—à Hand–® Hand–© Hand–™ Hand–´ Hand–¨ Hand–á Hand–ô Hand–à Hand—ò Hand—ô Hand–ª Hand–º Hand –º Hand–Ω Hand–æ Hand–ø HandœÄ HandœÅ HandœÉ HandœÇ Hand—ï Handzs Handzt Handdt Handdw Handdx Handdy Handdz Handdv Handzw Handwt Handwb Handwx Handwy Handzm Handzn Handzon Handzo Handza Handb Handbc Handcb Handdb Handdc Handdl Handel Handem Handen Hander Handes Hand√®s Hand—ç Handƒõ Hand–¥—å Hand—í–µ Handƒ° Hand—ú Hand‚ïù Hand—ã Hand–∑—ã Hand—Ü Hand—Ü–æ Hand—Ä–µ–∑ Hand–∂–∏ Hand–∑–∏ Hand–∑—å Hand–∑–∞ Hand–¥–∞ Hand–± Handa Handv Handu Handuv Handuw Handvy Handwm Handwn Handxd Handxe Handzy Handzz Handyt Handvt Handwd Handwq Handxb Handxc Handzd Handze Handzte Handft Handtf Handtg HandugHand ug Handuj Handuk Handul Handum Handun Handur Handus Handu√ü Hand—É Hand≈± HandÔøΩ Hand“Ø Hand—ó Hand–∏ Hand—ñ Hand–π Handzig HandŒ∂ Hand–∂–∞ Hand ê Hand–∂–µ Hand–∂–¥–µ Hand≈æe Hand–∂–¥–∞ Hand–Ω–¥ Hand–Ω–µ Hand–Ω–∏ Hand–Ω–æ Hand–ø–æ Hand–ø—É Hand–ø–∏ Hand—Ä–∏ Hand—Ä—ã Hand—Ä—å Hand—Ä Hand—Å Hand—Å—å Hand—Ç—å Hand—Å—Ç—å Hand—Ç Hand—Ç—É Hand—Ü—É Hand—Ü–∞ Hand–∑—è Hand–±–µ Hand–≤–µ Hand–≥–∞ Hand–≥–µ Hand“ë Hand–µ Hand—î Hand—ò–µ Hand—ò—É Handju Hand–∂—É Hand–∑–¥–∞ Hand–∑–¥ Hand–∑–µ Hand—Ç–µ Hand—Ç–≤–æ Hand tf HandUG HandUgHand UgB HandUB Handub Handvb Handvv Handww Handwh Handwc Handws Handxt Handqt HandXT HandXt Hand XtHand XT Hand Yt \n",
      "Was ist die Schl√ºssigkeit der Feststellung in Bezug II Ziv 109/96 des Beschlusses in dem vorliegenden Urteils des ¬ß 39 Absatz 5 des Gesetzes in den vorhergestellten Urteil des¬ß 60 Absatzen Urteleils in der vorjahrestellen Urteeils der ¬ß¬ß136 Abs√§tzen  Ur√§tel in vorhajrestelen Urt√©ilsder ¬ß16 √Ñ zen Urat√©l in Vorhaja≈ôtelen Urt√©lsder Vorjaj≈ôt√©len Urjaz√©l√©n in Vorgj√°z√©l√´n Urz√°z√©≈Ç√©n In V√∂rgj√°≈æ√©l√∂ in Worg≈æ√°≈º√©l√∏ in Wor≈æa≈æ√©ƒæ in Wo≈ôg≈æ√©l√©≈Ç in War≈æe≈æ√®le≈Ç –≤ War≈º√©–ª√´ –≤ W√∂r≈æƒõ–ª—î –≤ Wor≈ºƒõ≈Ç√© –≤ –í–æ—Ä—ä–∂—£–ª—ä –≤ –í–æ—Ä–∂ƒõ–ª—ë –≤ Vor–∂ƒó–ª—ë –≤ Voor≈æƒól—ë in For≈æƒõl≈ì in Fore≈æ√´l√∏ –≤ Forzƒõl√∏ In For≈ºƒõl√∏ Endschlie√ü in Endschel in endschl√∂ End≈æl≈ì End≈ºl√´ in √ândsch–ª≈ì –≤ √â–Ω–¥≈æ–ª—£ –≤ –≠–Ω–¥–∂–ªƒõ –≤ End í–ª—ç –≤ –ï–Ω–¥ í–ª—ë in –≠–Ω—ü–ªƒó –≤ –≠–Ω–∂–ª—ë End–∂–ª–µ –≤–≠–Ω–¥–∑–ªÔøΩƒõ in ”º–∂l√© –≤”Ω–∂–ª–µ–µ –≤“ò–∂–∑–ª—ë v“ô–∂—ô–µ –≤–ñ–∂–∂—ë v–ñ≈æ–ª–µ v≈Ω–∂≈æ–ª—ë–≤ –ñ–∂–∂–µ–ª–Å –≤ –ñ–æ—Ä íl√©–≤ ≈Ω–æ—Ä≈º–ª—ëv ≈Ωor≈ºl√© in ≈Ω√∂r í≈Ç—ë–≤ Z–æ—Ä–≥–∂√®le –≤ Z–æ—Ä—ä í–ª–µ–≤ –ó–æ—Ä–∑–∂—ë–ª–≤ z–û—Ä—ä–≥ íl—î–≤ –∑–û—Ä–≥–∂–µ–ª—ëw Zorg íle –≤ zOrg ílle –≤ Zag≈æl√©w Zag≈º–ª√®lew Za≈º–≥–ª—êw Azag í–ª–ª–µw Aza≈º≈Ç√®le–≤ Azorg–∂llew Argg íll√©w Arg íƒæ√© in Arg≈ælle Ar≈ºg≈Ç√´w Ang≈ºlle Ang≈æ≈Ç≈Çƒôw An≈æglle An≈º≈ºƒæƒô in Anz≈æƒæƒõw And≈º–ª–ª–µ –≤ Andz í‚Ñì√´–≤ And≈æ≈æ‚Ñì√© w And≈∫≈º‚Ñìƒô –≤ Anƒë≈º·∏∑–µ –≤ –ê–Ω–∂ í·∏∑—ç in –ê–Ω—ü–∂–ª–ª–µ –ê–Ω–∑“ë–ªƒô–≤ –ê–Ω—í–∂‚Ñì—ç–≤ ƒÑ–Ω–¥≈º–ª–µ in –ê–Ω–¥–∑—å–ªi√´ –≤ –ê–Ω—í–≥–∂–¥–µ –≤–ê–Ω–¥–≥≈æ–ª–ª–µ–≤ –ê–Ω–≥–∂–∂–¥–µ–≤ Angd≈ºelle –≤ Ang≈∫–∂–¥√®le in Angz ≈ºdelle–≤ An≈∫≈æd√®le w An ≈º≈ºd√´≈Çe –≤ A≈Ñ≈º–¥—£≈Ç–µ w A√±≈ºdzelle w Any≈ºdzie≈Ç√® w ≈É≈ºƒè√©ll√´ w –ê–Ω—å–∂dzie–ª√® –≤ Any–∂—í–ªƒï –≤ √Å–Ω—å≈º·∏ç–ª√™ –≤ √Ä–Ω—å≈æ–¥–∑—ñ–ª–ª–≤ √Ä–Ω–¥≈∫–¥–ª—ó –≤√Ä–Ω–¥—ä–∂·∏ç‚Ñì–≤√Ä–Ω–¥–∂–¥–∑‚Ñìv√Ä–Ω–≥—ü–¥–∑–µ–ª –≤√Å–Ω–≥–∑–¥–∂–µ–≤–ê–Ω–≥–¥–∂—ê–ªv√Å–Ω“ë–¥–∑—å–¥–∂–¥–µ–ª v√Å–Ω–¥“ë–∂—í–µ–ª–∑–≤√Å–¥“ë–∑–∂–¥–µ≈Ç–≤A–Ω–¥—ß–∂–∑–¥–∂–¥–µlv–ê–¥—©–∂ƒè–µ–ªz–≤”ß–¥–∂–∑–¥–∑dlv–ê–¥–∂–∂–¥—å–µ–∑lv”¶–¥–∂–∂–¥–µ–∑–ª—å–≤–≠–¥–∂—í–∂–¥–µ–ª–µ–≤–≠–¥≈º–∂–¥–µ–ª—ë–ª–≠—ü—í–∑–¥–∑–µ≈Çv–≠–∂–¥–∂dzie–≤√â–∂—ü–∑–¥–∑—ñ–≤–Å–∂–∂–¥–∞–∑–¥–∑–∏–ª–Ñ–∂–∂–¥—É–∑–¥–∑—ã–ª–à–∂–¥–∞–∑–¥zy–ª–å–∂–¥–∞—Ä–∑–¥–∑—è–ª–°–∂dar–∑–¥—è–ª–ó–∂–¥–∞—Ç–∑–¥—ä–ó–æ—Ä–¥–∂–∞–∑–¥–™–∂–¥–∞–Ω–∑–¥–´–∂–¥—ã–∑–¥–ó—ì–¥–∞—Ä–¥–∂–¥—ã–∑–î–∂—ó–¥–∞—Ä–¥–∑–¥–î—±–∂–¥–µ—Ä–∑–¥–ñ—°–¥–∞—Ä–∂–¥–∏–∑–ñ–∑–¥–∞—Ä–∂–∏–∑–ó“°–∞—Ä–∂–¥—ñ–∑“ü–∂–∞—Ä–¥–∂–¥–∏–ó—ä–∞—Ä–¥–∂–∏–¥–ó—ã–∂–¥–∏–∏–ó—å–∞—Ä–∂–∏–ª–¨–∂–∂–∏–ª—å–ó—é–∂–¥—è–ª–™≈æ–∞—Ä—ü–¥–∏–ª“•–∂–¥–∏—è–∑–¥“ß≈æ–¥–∞—Ä—ü–∏–ª”•—ü–¥—ã–∂“µ–∂–¥–∏–Ω–∑–¥”°–∂—ã–∑–¥—•–¥–∂—ã–∑”¢–∂–∑—ã–∑–¥–•–¥–∑–∏–∑–¥–û–∂–∑–¥–∞–∑–´–¥–∑—ã–¥–´–∑–û—¢–∑–¥–∂–∏–∑–¥–Å–∑–¥—ã–∑–¥–∞–ª–ñ—å–¥–∞—Ä–∂–∏–≤–∞–∑–¥–∂–î—ã–∑–¥–∞–ª—å–ñ“ó–¥–∞—Ä–∂–∏–≤–∞–¥≈æ–î—å–∑–¥–∂–∞–ª–∂–ñ—ä–¥–∞—Ä–∂–∞–¥–ñ—ã–∂–¥–∞–ªzh–ñ–∞–¥–∑—è–¥zh≈Ω–∞–∑–¥≈æ—è–¥Z–∂–∞–∑–¥–∞–∂Z—Ö–∞—Ä—í–∞–∑Z–¥—ä–∞–∂–¥–∞–∂–ó—Ö—ä–¥arz–∂dZh–∞dz–∞–∂dzZhdZ≈æ–∞—ü–∞≈º\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print(train_dataset[501][\"rejected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95350ef9-6568-4ee8-bee7-d6fec8c4f502",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bdde3d88fec40b887a57ee9c8d20143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22040 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8b959fbe2c84a278e911067203f5fc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/22040 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['chosen', 'rejected', 'input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected'],\n",
      "    num_rows: 22040\n",
      "})\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    ")\n",
    "train_dataset = train_dataset.filter(\n",
    "    lambda x: len(x[\"input_ids_chosen\"]) <= 512\n",
    "    and len(x[\"input_ids_rejected\"]) <= 512\n",
    ")\n",
    "\n",
    "print(train_dataset)\n",
    "print(type(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9a3c8be-ff88-43d9-839a-0810c56c02d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98740d14-e03f-4f62-9c2a-2a4e1ddd858f",
   "metadata": {},
   "source": [
    "# RewardTrainer benutzen, um unser Modell zu verbessern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b92913-d537-45ec-b461-dba7ba41e827",
   "metadata": {},
   "source": [
    "## Model GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee19c1b-48ec-43cc-8769-22a85af39f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have to transform the parameters in our lora adapters, so they can be changed by the reward trainer\n",
    "# the code enables gradient tracking for all parameters\n",
    "# but first they have to be floating-point type (like torch.float32), which is required for gradients to be computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf9a7315-aacd-472b-8b93-4115570b5af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in peft_model_gpt.parameters():\n",
    "    param.data = param.data.float()  # Convert to float if not already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "817108f8-ea98-4a71-a7c7-98afb6bb005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in peft_model_gpt.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45606105-743d-45f1-812e-6a31b097795c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in peft_model_gpt.parameters():\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "323d3ec4-fcea-4f9e-a27d-7c5f173655d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "\n",
    "# set the wandb project where this run will be logged\n",
    "os.environ[\"WANDB_PROJECT\"]=\"sauLLM\"\n",
    "\n",
    "# save your trained model checkpoint to wandb\n",
    "os.environ[\"WANDB_LOG_MODEL\"]=\"checkpoint\"\n",
    "\n",
    "# turn off watch to log faster\n",
    "os.environ[\"WANDB_WATCH\"]=\"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27480af9-d3a4-42f9-a600-9957501a61a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 22,040 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 2\n",
      "\\        /    Total batch size = 8 | Total steps = 100\n",
      " \"-____-\"     Number of trainable parameters = 3,800,305,664\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mm-schmerle\u001b[0m (\u001b[33mm-schmerle-universit-t-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ps2024/sauLLM/reinforcement_learning/wandb/run-20241102_145504-onr6n6ef</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/m-schmerle-universit-t-/sauLLM/runs/onr6n6ef' target=\"_blank\">./model_gpt/train_logs</a></strong> to <a href='https://wandb.ai/m-schmerle-universit-t-/sauLLM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/m-schmerle-universit-t-/sauLLM' target=\"_blank\">https://wandb.ai/m-schmerle-universit-t-/sauLLM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/m-schmerle-universit-t-/sauLLM/runs/onr6n6ef' target=\"_blank\">https://wandb.ai/m-schmerle-universit-t-/sauLLM/runs/onr6n6ef</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 1:22:25, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./model_gpt/train_logs/checkpoint-20)... Done. 4.5s\n",
      "/home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./model_gpt/train_logs/checkpoint-40)... Done. 4.6s\n",
      "/home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./model_gpt/train_logs/checkpoint-60)... Done. 4.6s\n",
      "/home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./model_gpt/train_logs/checkpoint-80)... Done. 4.6s\n",
      "/home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./model_gpt/train_logs/checkpoint-100)... Done. 4.6s\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from peft import LoraConfig\n",
    "from trl import RewardTrainer, RewardConfig\n",
    "\n",
    "# Step 1: Set up reward and LoRA configurations\n",
    "reward_config = RewardConfig(\n",
    "    output_dir=\"./model_gpt/train_logs\",\n",
    "    max_steps=100,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1.41e-5,\n",
    "    optim=\"adamw_torch\",\n",
    "    save_steps=20,  # Save checkpoints during training to monitor, but we'll keep only the final model\n",
    "    logging_steps=20,\n",
    "    report_to=\"wandb\",\n",
    "    remove_unused_columns=False,\n",
    "    max_length=512,\n",
    "    gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    inference_mode=False, # has to be false or else the reward trainer can't adjust the parameters\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0\n",
    ")\n",
    "\n",
    "# Step 2: Initialize RewardTrainer with your LoRA-enhanced model\n",
    "reward_trainer = RewardTrainer(\n",
    "    model=peft_model_gpt,  \n",
    "    tokenizer=tokenizer,\n",
    "    args=reward_config,\n",
    "    train_dataset=train_dataset,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "# Step 3: Train the model with reward feedback\n",
    "reward_trainer.train()\n",
    "\n",
    "# Step 4: Save only the improved model\n",
    "reward_trainer.model.save_pretrained(\"./improved_peft_model_gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb4a43b5-985b-4c0a-9c51-7154ba257242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.9.post4: Fast Mistral patching. Transformers = 4.45.1.\n",
      "   \\\\   /|    GPU: Quadro RTX 6000. Max memory: 23.462 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.1. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Model and Tokenizer Loading\n",
    "base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/mistral-7b-v0.3\", \n",
    "    local_files_only=True, \n",
    "    max_seq_length=2048, \n",
    "    dtype=None, \n",
    "    load_in_4bit=True \n",
    ")\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(base_model, \"../reinforcement_learning/improved_peft_model_gpt/\") \n",
    "\n",
    "FastLanguageModel.for_inference(peft_model)\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd9c5764-fe8f-4c4d-aa25-fd2d26a5f07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Was ist Mord?\"\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(\n",
    "    input_text,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=tokenizer.model_max_length\n",
    ").to(device)\n",
    "\n",
    "# Generate output\n",
    "with torch.no_grad():\n",
    "    outputs = peft_model.generate(\n",
    "        **inputs,\n",
    "        max_length=2048,\n",
    "        repetition_penalty=1.2,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a4496d6-bc50-4f36-8be3-5dfbfbd3aedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Was ist Mord?\n"
     ]
    }
   ],
   "source": [
    "# Decode the output\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1549604-9459-4db5-ade3-826ad1aea866",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
