{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a86293-1d95-4b1d-96fa-decb2612d7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackabuse.com/reading-and-writing-xml-files-in-python-with-pandas/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f23e7729-72a3-4564-9ce8-23560f895ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import unicodedata\n",
    "from lxml import etree\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Directory paths\n",
    "input_dir = './bgh_urteile/'\n",
    "output_dir = './normalized_bgh_urteile/'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1ea57291-708b-4ecf-9571-4eb2b5cb7cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize text using unicodedata\n",
    "def normalize_text(text):\n",
    "    return unicodedata.normalize('NFKC', text)\n",
    "\n",
    "# Function to create 10-grams from text\n",
    "def create_ngrams(text, n=10):\n",
    "    words = text.split()\n",
    "    ngrams = [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "    return ngrams\n",
    "\n",
    "# Function to extract all text content from an XML element\n",
    "def extract_text_and_elements(root):\n",
    "    texts = []\n",
    "    elements = []\n",
    "    for elem in root.iter():\n",
    "        if elem.text:\n",
    "            texts.append(elem.text.strip())\n",
    "            elements.append(elem)\n",
    "    return texts, elements\n",
    "\n",
    "# Function to remove duplicated 10-grams from text\n",
    "def remove_duplicated_ngrams(text, duplicated_ngrams, ngram_tracker):\n",
    "    deleted_ngrams = []\n",
    "    for ngram in duplicated_ngrams:\n",
    "        count = ngram_tracker[ngram]\n",
    "        occurrences = text.count(ngram)\n",
    "        if occurrences > 1:\n",
    "            text = text.replace(ngram, '', occurrences - 1)\n",
    "            deleted_ngrams.append(ngram)\n",
    "    return text, deleted_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1c1bd653-0b55-47b6-b076-ebef56a840d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted n-grams in file bgh_20100223_XI-ZR-190-09.xml:\n",
      "Gesetzes zur Umsetzung der Verbraucherkreditrichtlinie, des zivilrechtlichen Teils der Zahlungsdiensterichtlinie\n",
      "Deleted n-grams in file bgh_20100211_III-ZR-12-09.xml:\n",
      "dass die im Prospekt werbend herausgestellte Mittelverwendungskontrolle bislang nicht stattgefunden\n",
      "Deleted n-grams in file bgh_20100112_3-StR-439-09.xml:\n",
      "so schon Tröndle/Fischer, StGB 49. Aufl. (1999) § 66 Rdn.\n",
      "Deleted n-grams in file bgh_20100211_III-ZR-11-09.xml:\n",
      "dass die im Prospekt werbend herausgestellte Mittelverwendungskontrolle bislang nicht stattgefunden\n",
      "Deleted n-grams in file bgh_20100222_II-ZR-287-07.xml:\n",
      "ob die Übereignung der Zylinder von der Beklagten zu 1\n",
      "Deleted n-grams in file bgh_20100211_III-ZR-7-09.xml:\n",
      "dass die im Prospekt werbend herausgestellte Mittelverwendungskontrolle bislang nicht stattgefunden\n",
      "Deleted n-grams in file bgh_20100218_IX-ZA-39-09.xml:\n",
      "Beschl. v. 16. Juli 2009 aaO S. 1779 f Rn.\n",
      "Deleted n-grams in file bgh_20100113_3-StR-507-09.xml:\n",
      "(vgl. BGH Urteil vom 9. April 2009 - 3 StR\n",
      "Beschluss vom 4. August 2009 - 3 StR 174/09 -\n",
      "Deleted n-grams in file bgh_20100218_4-ARs-16-09.xml:\n",
      "Sonnenberger, in: Münchener Kommentar zum BGB, 4. Auflage 2006, Einleitung,\n",
      "Deleted n-grams in file bgh_20100211_III-ZR-10-09.xml:\n",
      "dass die im Prospekt werbend herausgestellte Mittelverwendungskontrolle bislang nicht stattgefunden\n",
      "Deleted n-grams in file bgh_20100107_AnwZ-(B)-79-09.xml:\n",
      "Beschl. v. 7. Dezember 2004, AnwZ (B) 40/04, NJW 2005,\n",
      "Beschl. v. 7. März 2005, AnwZ (B) 7/04, NJW 2005,\n",
      "Deleted n-grams in file bgh_20100224_1-StR-260-09.xml:\n",
      "nach § 243 Abs. 3 Satz 1 StPO zu verlesen\n",
      "§ 243 Abs. 3 Satz 1 StPO i.V.m. § 200\n",
      "Beschl. vom 25. November 2009 - 2 ARs 445/09 -\n",
      "Einführung der Anklage in entsprechender Anwendung des § 249 Abs.\n",
      "Deleted n-grams in file bgh_20100114_4-StR-450-09.xml:\n",
      "in Tateinheit mit gefährlichem Eingriff in den Straßenverkehr, des versuchten\n",
      "Deleted n-grams in file bgh_20100113_IV-ZR-28-09.xml:\n",
      "Obliegenheit aus § 21 Nr. 1 Buchst. c VHB 92\n",
      "Deleted n-grams in file bgh_20100114_4-StR-93-09.xml:\n",
      "des Ausspähens von Daten in Tateinheit mit gewerbs- und bandenmäßiger\n",
      "Deleted n-grams in file bgh_20100211_III-ZR-9-09.xml:\n",
      "dass die im Prospekt werbend herausgestellte Mittelverwendungskontrolle bislang nicht stattgefunden\n",
      "Deleted n-grams in file bgh_20100223_4-StR-599-09.xml:\n",
      "Beschl. vom 11. November 2004 - 5 StR 299/03, BGHSt\n",
      "Normalization and duplicate removal completed for all files.\n"
     ]
    }
   ],
   "source": [
    "for root, dirs, files in os.walk(input_dir):\n",
    "    for filename in files:\n",
    "        if filename.endswith('.xml') and 'checkpoint' not in filename:\n",
    "            input_path = os.path.join(root, filename)\n",
    "            normalized_filename = f\"normalized_{filename}\"\n",
    "            relpath = os.path.relpath(root, input_dir)\n",
    "            output_subdir = os.path.join(output_dir, relpath)\n",
    "            os.makedirs(output_subdir, exist_ok=True)\n",
    "            output_path = os.path.join(output_subdir, normalized_filename)\n",
    "            \n",
    "            # Parse the XML file\n",
    "            tree = etree.parse(input_path)\n",
    "            root_element = tree.getroot()\n",
    "\n",
    "            # Normalize text in XML elements\n",
    "            for element in root_element.iter():\n",
    "                if element.text:\n",
    "                    element.text = normalize_text(element.text)\n",
    "\n",
    "            # Extract normalized text and elements\n",
    "            normalized_texts, normalized_elements = extract_text_and_elements(root_element)\n",
    "            \n",
    "            # Create 10-grams from normalized text\n",
    "            all_ngrams = []\n",
    "            for text in normalized_texts:\n",
    "                all_ngrams.extend(create_ngrams(text))\n",
    "\n",
    "            # Count the 10-grams to find duplicates\n",
    "            ngram_counts = Counter(all_ngrams)\n",
    "            \n",
    "            # Find duplicated 10-grams\n",
    "            duplicated_ngrams = [ngram for ngram, count in ngram_counts.items() if count > 1]\n",
    "\n",
    "            # Track deleted n-grams\n",
    "            deleted_ngrams = []\n",
    "\n",
    "            # Initialize n-gram tracker to track deletions\n",
    "            ngram_tracker = ngram_counts.copy()\n",
    "\n",
    "            # Remove duplicated 10-grams from normalized text elements\n",
    "            for element in normalized_elements:\n",
    "                if element.text:\n",
    "                    element.text, deleted = remove_duplicated_ngrams(element.text, duplicated_ngrams, ngram_tracker)\n",
    "                    deleted_ngrams.extend(deleted)\n",
    "\n",
    "            # Save the modified XML file\n",
    "            tree.write(output_path, pretty_print=True, xml_declaration=True, encoding='UTF-8')\n",
    "\n",
    "            # Print deleted n-grams\n",
    "            if deleted_ngrams:\n",
    "                print(f\"Deleted n-grams in file {filename}:\")\n",
    "                for ngram in deleted_ngrams:\n",
    "                    print(ngram)\n",
    "\n",
    "print(\"Normalization and duplicate removal completed for all files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57e380b-6d1f-4078-93b7-a80019847574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Display the 10-grams\n",
    "df_ngrams = pd.DataFrame(all_ngrams, columns=['10-Gram'])\n",
    "# Set display options to show all columns and rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "display(df_ngrams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
