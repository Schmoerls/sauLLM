{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a262a64-24d3-4392-93bd-a8d45bcd9340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erste Umsetzung des Finetuningprozesses in Form eines Jupyter Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8923044d-4189-47e2-bf61-bdff507c5d50",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-n3kh3n0g/unsloth_ae653a873e6c46249cbbf2b9bc9e683b\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-n3kh3n0g/unsloth_ae653a873e6c46249cbbf2b9bc9e683b\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit ae9e264e33c69b53dd5d533a4c5a264af4141c28\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (24.1)\n",
      "Requirement already satisfied: tyro in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.8.11)\n",
      "Requirement already satisfied: transformers<4.45.0 in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.31.0)\n",
      "Requirement already satisfied: datasets>=2.16.0 in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.1)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.0)\n",
      "Requirement already satisfied: tqdm in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.66.5)\n",
      "Requirement already satisfied: psutil in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (5.9.0)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.44.0)\n",
      "Requirement already satisfied: numpy in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.0.1)\n",
      "Requirement already satisfied: protobuf<4.0.0 in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.25.1)\n",
      "Requirement already satisfied: hf-transfer in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.8)\n",
      "Requirement already satisfied: filelock in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.10.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from huggingface-hub->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.11.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from transformers<4.45.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from transformers<4.45.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from transformers<4.45.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.4.5)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (13.9.1)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.7.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.13.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.8.30)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.15.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.16.0)\n",
      "Requirement already satisfied: xformers in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (0.0.28.post1)\n",
      "Requirement already satisfied: trl in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (0.5.0)\n",
      "Requirement already satisfied: peft in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (0.4.0)\n",
      "Requirement already satisfied: accelerate in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (0.21.0)\n",
      "Requirement already satisfied: bitsandbytes in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (0.40.2)\n",
      "Requirement already satisfied: triton in /home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages (3.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "transformers\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "datasets\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "scipy\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "ipywidgets\n"
     ]
    }
   ],
   "source": [
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "# We have to check which Torch version for Xformers (2.3 -> 0.0.27)\n",
    "from torch import __version__; from packaging.version import Version as V\n",
    "xformers = \"xformers==0.0.27\" if V(__version__) < V(\"2.4.0\") else \"xformers\"\n",
    "!pip install --no-deps {xformers} trl peft accelerate bitsandbytes triton\n",
    "\n",
    "%pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "print(\"transformers\")\n",
    "# Für Lora Finetuning benötigt\n",
    "# Sonstige Visualisierungs &\n",
    "%pip install -q -U datasets\n",
    "print(\"datasets\")\n",
    "%pip install -q -U scipy\n",
    "print(\"scipy\")\n",
    "%pip install -q -U ipywidgets\n",
    "print(\"ipywidgets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bbd2134-3ebb-4df1-803b-adce49c25618",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "# We have to check which Torch version for Xformers (2.3 -> 0.0.27)\n",
    "from torch import __version__; from packaging.version import Version as V\n",
    "xformers = \"xformers==0.0.27\" if V(__version__) < V(\"2.4.0\") else \"xformers\"\n",
    "!pip install --no-deps {xformers} trl peft accelerate bitsandbytes triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24591274-9935-4670-a8a2-1088e95f5492",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "460d6b35-6c2e-40bd-8cc2-2c15cef27416",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/ps2024/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login, logout\n",
    "\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88fe3b1f-6a05-41c4-8578-6378ad3da79c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth 2024.9.post1: Fast Mistral patching. Transformers = 4.45.0.dev0.\n",
      "   \\\\   /|    GPU: Quadro RTX 6000. Max memory: 23.644 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.0. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.27.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.9.post1 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/mistral-7b-v0.3\", \n",
    "    max_seq_length = 2048, # Choose any! We auto support RoPE Scaling internally!\n",
    "    dtype = None, # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "    load_in_4bit = True, # Use 4bit quantization to reduce memory usage. Can be False.\n",
    ")\n",
    "\n",
    "model_with_peft = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "\n",
    "\n",
    "def formatting_func_1(example):\n",
    "    text = f\"### Question: {example['input']}\\n ### Answer: {example['output']}\"\n",
    "    return text\n",
    "\n",
    "max_length = 8000 # kann verändert werden\n",
    "\n",
    "def generate_and_tokenize_prompt(prompt):\n",
    "    result = tokenizer(\n",
    "        formatting_func_1(prompt),\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "858371d5-4fdf-4672-87cd-4f6c4b6d5677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################################################\n",
      "Example after read & split: {'input': '', 'output': 'Die Berufung der Klägerin gegen das am 12.01.2000 verkündete Urteil der 6. \n",
      "#########################################################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb2397a64af64d8ab69b3c197c9efd35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/95397 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065ebfe52c284f53bf6129c6a8972ea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example after tokenize: {'input': '', 'output': 'Die Berufung der Klägerin gegen das am 12.01.2000 verkündete Urteil der 6. \n",
      "#########################################################\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "json_paths = ['data/th.json', \n",
    "              'data/st.json', \n",
    "              'data/sh.json', \n",
    "              'data/rp.json', \n",
    "              'data/mv.json', \n",
    "              'data/hh.json', \n",
    "              'data/he.json', \n",
    "              'data/bund.json', \n",
    "              'data/be.json', \n",
    "              'data/bb.json', \n",
    "              'data/sl.json']\n",
    "\n",
    "dataset = load_dataset('json', data_files=json_paths, split='train')\n",
    "\n",
    "# 90-10 Split\n",
    "train_test_split = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']\n",
    "\n",
    "print(\"#\" * 120)\n",
    "print(\"Example after read & split: \" + str(train_dataset[0])[:300])\n",
    "print(\"#########################################################\")\n",
    "\n",
    "# Tokenize the datasets\n",
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = test_dataset.map(generate_and_tokenize_prompt)\n",
    "\n",
    "# Convert the tokenized example to a string before printing\n",
    "print(\"Example after tokenize: \" + str(tokenized_train_dataset[0])[:300])\n",
    "print(\"#\" * 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "794c81cf-8aef-4096-a4db-16555bdf1bdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch;\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a72f342-552a-4183-bde3-912ed0ab62b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorchVersion2.4.1\n",
      "Free memory: 24045.44 MB (23.48 GB)\n",
      "Total memory: 24211.31 MB (23.64 GB)\n"
     ]
    }
   ],
   "source": [
    "# Freien Speicher anzeigen\n",
    "import torch\n",
    "print(\"PyTorchVersion\" + torch.__version__)\n",
    "\n",
    "free_mem, total_mem = torch.cuda.mem_get_info()\n",
    "\n",
    "free_mem_MB = free_mem / (1024 ** 2)\n",
    "total_mem_MB = total_mem / (1024 ** 2)\n",
    "\n",
    "free_mem_GB = free_mem / (1024 ** 3)\n",
    "total_mem_GB = total_mem / (1024 ** 3)\n",
    "\n",
    "print(f\"Free memory: {free_mem_MB:.2f} MB ({free_mem_GB:.2f} GB)\")\n",
    "print(f\"Total memory: {total_mem_MB:.2f} MB ({total_mem_GB:.2f} GB)\")\n",
    "#print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e3a5d8e-50d8-4815-a87b-b847f16e5060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################################################################################################\n",
      "Running training with...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m120\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning training with...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mtokenized_train_dataset\u001b[49m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m training tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tokenized_val_dataset), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m test tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m120\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"#\" * 120)\n",
    "print(\"Running training with...\")\n",
    "print(len(tokenized_train_dataset), \" training tokens\")\n",
    "print(len(tokenized_val_dataset), \" test tokens\")\n",
    "print(\"#\" * 120)\n",
    "\n",
    "output_dir = \"./loras\"\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model_with_peft,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=100, # Sollten ungefähr 5 - 10% dre max steps sein oder?\n",
    "        per_device_train_batch_size=2, # Besser 2\n",
    "        gradient_accumulation_steps=1, # Besser 1\n",
    "        gradient_checkpointing=True,\n",
    "        max_steps=1000, # Besser > 400\n",
    "        learning_rate=2.5e-5, # Want a small lr for finetuning maybe 1e-5?\n",
    "        fp16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_steps=100,              # When to start reporting loss\n",
    "        logging_dir=\"./logs\",        # Directory for storing logs\n",
    "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "        save_steps=100,                # Save checkpoints every 50 steps\n",
    "        eval_strategy=\"steps\", # Evaluate the model every logging step\n",
    "        eval_steps=100,               # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,                # Perform evaluation at the end of training\n",
    "        #report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n",
    "        #run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    # compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"lora_model\") # Local saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbb6dc2b-b599-4e49-841e-a5c93b5477ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b05ff9af9c4defa2ec318b99f759f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/105997 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "json_paths = ['data/th.json', \n",
    "              'data/st.json', \n",
    "              'data/sh.json', \n",
    "              'data/rp.json', \n",
    "              'data/mv.json', \n",
    "              'data/hh.json', \n",
    "              'data/he.json', \n",
    "              'data/bund.json', \n",
    "              'data/be.json', \n",
    "              'data/bb.json', \n",
    "              'data/sl.json']\n",
    "\n",
    "dataset = load_dataset('json', data_files=json_paths, split='train')\n",
    "\n",
    "prompt = \"\"\"\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Answer:\n",
    "{}\n",
    "\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(example):\n",
    "    exinput = example['input']\n",
    "    exoutput = example['output']\n",
    "    texts = []\n",
    "\n",
    "    for i,j  in zip(exinput, exoutput):\n",
    "        text = prompt.format(i,j) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37ea2a3a-27da-483d-8f6f-8d2feb95c90f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': '', 'output': 'Gegen den Betroffenen wird wegen Überschreitung der zulässigen Höchstgeschwindigkeit innerhalb geschlossener Ortschaften um 24 km/h eine Geldbuße von 80 Euro verhängt. Der Betroffene trägt die Kosten des Verfahrens sowie seine notwendigen Auslagen. Angewandte Vorschriften: §§\\xa03 Abs.\\xa03, 49 Abs.\\xa01 Nr.\\xa03 StVO, 24 StVG, 46 Abs.\\xa01 OWiG, 465 Abs.\\xa01 StPO, Nr.\\xa011.3.4 Anlage BKatV I. Der Betroffene ist 50 Jahre alt und hat gemäß Auszug aus dem Fahreignungsregister vom 01.02.2017 eine Eintragung hierin: Am … erging durch die zentrale Bußgeldstelle des Landes Brandenburg ein Bußgeldbescheid gegen den Betroffenen wegen Überschreitung der zulässigen Höchstgeschwindigkeit außerhalb geschlossener Ortschaften um 38 km/h als Führer eines PKW am … um … Uhr. Gegen ihn wurde eine Geldbuße von 120,- € verhängt. Die Entscheidung ist rechtskräftig seit dem 14.05.2015, Aktenzeichen: …. II. Am 21.01.2016 um 07.47 Uhr fuhr der Betroffene in der Ortsdurchfahrt S…, L 1361, mit dem PKW, amtliches Kennzeichen: …-…, innerhalb geschlossener Ortschaften mit überhöhter Geschwindigkeit von 74 km/h. Es war die allgemein für Ortschaften zulässige Höchstgeschwindigkeit von 50 km/h gemäß §\\xa03 Abs.\\xa03 StVO gültig. Die Überschreitung beträgt somit 24 km/h. III. Dieser Sachverhalt steht fest aufgrund der Beweisaufnahme in der Hauptverhandlung. Der Betroffene hat sich nicht zur Sache eingelassen. Der Messbeamte wurde als Zeuge vernommen und schilderte den Betrieb des Messgerätes. Hierbei wurde ersichtlich, dass das Messgerät ordnungsgemäß aufgestellt und betrieben wurde und hierbei keinerlei Fehler auftraten. Die Befähigung des Messbeamten zur Bedienung des Messgerätes ergibt sich aus der Bescheinigung vom 09.05.2014 (Bl. 9 d.A.), die in der Hauptverhandlung verlesen wurde. Die Lichtbilder der Messung (Bl. 10 f. d.A.) wurden in der Hauptverhandlung in Augenschein genommen. Die Informationen über die konkrete Messung (Bl. 10 d.A) wurden in der Hauptverhandlung verlesen. Hieraus ergeben sich insbesondere die Zeit der Handlung und die gemessene Geschwindigkeit. Das Geschwindigkeitsmessblatt vom 21.01.2016 (Bl. 5 f. d.A.) wurde in der Hauptverhandlung verlesen. Hieraus ergeben sich die Details über die Messsituation und den Betrieb des Messgerätes sowie insbesondere der Ort der Handlung und die Prüfung der Unversehrtheit der Eichmarken. Der Eichschein des Messgerätes vom 03.07.2015 (Bl. 7 f. d.A.) wurde in der Hauptverhandlung verlesen. Hieraus ergeben sich die Toleranzabzüge (Verkehrsfehlergrenzen) sowie die Tatsache, dass das Messgerät vom Landesamt für Mess- und Eichwesen Berlin-Brandenburg am 02.07.2015 geeicht worden ist. Damit wurde bestätigt, dass das Messgerät den Anforderungen des MessEG, der MessEV und des Prüfverfahrens entsprechend der PTB – Prüfverfahren der Bauartzulassung „Richtlinie zur Eichung der Geschwindigkeit Überwachungsanlagen der Typen PoliScan speed, PoliScan speed F1 und PoliScan M1 HP“ – entspricht. Zudem ergibt sich hieraus, dass die Eichung bis Ende 2016 gültig war. Der Toleranzabzug ist bei der vorliegenden Messung ordnungsgemäß in Ansatz gebracht worden. Der Beweisantrag des Verteidigers auf Einholung eines Sachverständigengutachtens sowie Beiziehung von Messdatei, Token und Passwort, den dieser in der Hauptverhandlung wiederholte (Bl. 53, 84 d.A.), wurde nach §\\xa077 Abs.\\xa02 Nr. OWiG abgelehnt. Diese Beweiserhebungen waren nach pflichtgemäßem Ermessen des Gerichts zur Erforschung der Wahrheit nicht erforderlich. Die Ordnungswidrigkeit war bereits aufgrund der in die Hauptverhandlung eingeführten Beweismittel nachgewiesen. Es handelt sich um ein standardisiertes Messverfahren. Dies ergibt sich aus der Eichung des Messgerätes durch die zuständige Behörde und der Einhaltung aller Bedingungen einer ordnungsgemäßen Messung. Konkrete Tatsachen, die Zweifel an der Richtigkeit des Messergebnisses begründen könnten, wurden seitens der Verteidigung nicht vorgetragen. Doch wäre auch, wenn man hier nicht von einem standardisierten Messverfahren ausgehen würde, die Ordnungswidrigkeit durch die in die Hauptverhandlung eingeführten Beweismittel nachgewiesen. Im Übrigen ist anzumerken, dass ein solches Sachverständigengutachten, wie es beantragt wurde, nicht geeignet ist, die behauptete Tatsache zu beweisen. Ein Sachverständiger kann lediglich hinsichtlich der technischen Vorgänge bewerten, ob das Messergebnis richtig gewonnen wurde und physikalisch korrekt ist. Dies wurde von der Verteidigung nicht angezweifelt. Fragen der Einhaltung der Bauartzulassung sind Rechtsfragen. Hierzu hat sich ein Sachverständiger nicht zu verhalten, sondern dies ist vom Gericht zu beurteilen. Zweifel an der Einhaltung der Bauartzulassung sind jedoch nach Feststellung des Gerichts in der Hauptverhandlung aufgrund der durchgeführten Beweisaufnahme nicht gegeben. Die Bauartzulassung enthält Vorgaben, die Messgeräte der vorliegenden Art im Allgemeinen erfüllen müssen. Sie ist Grundlage für die Eichung (vgl. §§\\xa03 Nr.\\xa03, 27 MessEG). Auf dem Eichschein vom 03.07.2015 hat das Landesamt für Mess- und Eichwesen Berlin-Brandenburg bestätigt, dass das Gerät geeicht wurde und somit den Anforderungen der Bauartzulassung entspricht. Messdatei, Token und Passwort werden für den Nachweis der Ordnungswidrigkeit nicht benötigt. IV. Diese Ordnungswidrigkeit kann mit einer Geldbuße bis 2.000 € geahndet werden (§\\xa024 Abs.\\xa02 StVG). Da jedoch von Fahrlässigkeit ausgegangen werden muss, da ein Vorsatz nicht nachweisbar ist, reduziert sich dieser Rahmen auf maximal 1.000 € gemäß §\\xa017 Abs.\\xa02 OWiG. Vorliegend wurde die Regelgeldbuße von 80 € gemäß Nr.\\xa011.3.4 Anlage BKatV für handlungs- und schuldangemessen erachtet. Dabei war von einem durchschnittlichen Verschulden und einer durchschnittlichen Schwere der Ordnungswidrigkeit auszugehen. Gründe für ein Abweichen von der Regelgeldbuße lagen nicht vor. V. Die Kostenentscheidung folgt aus §§\\xa046 Abs.\\xa01 OWiG, 465 Abs.\\xa01 StPO.', 'text': '\\n\\n### Question:\\n\\n\\n### Answer:\\nGegen den Betroffenen wird wegen Überschreitung der zulässigen Höchstgeschwindigkeit innerhalb geschlossener Ortschaften um 24 km/h eine Geldbuße von 80 Euro verhängt. Der Betroffene trägt die Kosten des Verfahrens sowie seine notwendigen Auslagen. Angewandte Vorschriften: §§\\xa03 Abs.\\xa03, 49 Abs.\\xa01 Nr.\\xa03 StVO, 24 StVG, 46 Abs.\\xa01 OWiG, 465 Abs.\\xa01 StPO, Nr.\\xa011.3.4 Anlage BKatV I. Der Betroffene ist 50 Jahre alt und hat gemäß Auszug aus dem Fahreignungsregister vom 01.02.2017 eine Eintragung hierin: Am … erging durch die zentrale Bußgeldstelle des Landes Brandenburg ein Bußgeldbescheid gegen den Betroffenen wegen Überschreitung der zulässigen Höchstgeschwindigkeit außerhalb geschlossener Ortschaften um 38 km/h als Führer eines PKW am … um … Uhr. Gegen ihn wurde eine Geldbuße von 120,- € verhängt. Die Entscheidung ist rechtskräftig seit dem 14.05.2015, Aktenzeichen: …. II. Am 21.01.2016 um 07.47 Uhr fuhr der Betroffene in der Ortsdurchfahrt S…, L 1361, mit dem PKW, amtliches Kennzeichen: …-…, innerhalb geschlossener Ortschaften mit überhöhter Geschwindigkeit von 74 km/h. Es war die allgemein für Ortschaften zulässige Höchstgeschwindigkeit von 50 km/h gemäß §\\xa03 Abs.\\xa03 StVO gültig. Die Überschreitung beträgt somit 24 km/h. III. Dieser Sachverhalt steht fest aufgrund der Beweisaufnahme in der Hauptverhandlung. Der Betroffene hat sich nicht zur Sache eingelassen. Der Messbeamte wurde als Zeuge vernommen und schilderte den Betrieb des Messgerätes. Hierbei wurde ersichtlich, dass das Messgerät ordnungsgemäß aufgestellt und betrieben wurde und hierbei keinerlei Fehler auftraten. Die Befähigung des Messbeamten zur Bedienung des Messgerätes ergibt sich aus der Bescheinigung vom 09.05.2014 (Bl. 9 d.A.), die in der Hauptverhandlung verlesen wurde. Die Lichtbilder der Messung (Bl. 10 f. d.A.) wurden in der Hauptverhandlung in Augenschein genommen. Die Informationen über die konkrete Messung (Bl. 10 d.A) wurden in der Hauptverhandlung verlesen. Hieraus ergeben sich insbesondere die Zeit der Handlung und die gemessene Geschwindigkeit. Das Geschwindigkeitsmessblatt vom 21.01.2016 (Bl. 5 f. d.A.) wurde in der Hauptverhandlung verlesen. Hieraus ergeben sich die Details über die Messsituation und den Betrieb des Messgerätes sowie insbesondere der Ort der Handlung und die Prüfung der Unversehrtheit der Eichmarken. Der Eichschein des Messgerätes vom 03.07.2015 (Bl. 7 f. d.A.) wurde in der Hauptverhandlung verlesen. Hieraus ergeben sich die Toleranzabzüge (Verkehrsfehlergrenzen) sowie die Tatsache, dass das Messgerät vom Landesamt für Mess- und Eichwesen Berlin-Brandenburg am 02.07.2015 geeicht worden ist. Damit wurde bestätigt, dass das Messgerät den Anforderungen des MessEG, der MessEV und des Prüfverfahrens entsprechend der PTB – Prüfverfahren der Bauartzulassung „Richtlinie zur Eichung der Geschwindigkeit Überwachungsanlagen der Typen PoliScan speed, PoliScan speed F1 und PoliScan M1 HP“ – entspricht. Zudem ergibt sich hieraus, dass die Eichung bis Ende 2016 gültig war. Der Toleranzabzug ist bei der vorliegenden Messung ordnungsgemäß in Ansatz gebracht worden. Der Beweisantrag des Verteidigers auf Einholung eines Sachverständigengutachtens sowie Beiziehung von Messdatei, Token und Passwort, den dieser in der Hauptverhandlung wiederholte (Bl. 53, 84 d.A.), wurde nach §\\xa077 Abs.\\xa02 Nr. OWiG abgelehnt. Diese Beweiserhebungen waren nach pflichtgemäßem Ermessen des Gerichts zur Erforschung der Wahrheit nicht erforderlich. Die Ordnungswidrigkeit war bereits aufgrund der in die Hauptverhandlung eingeführten Beweismittel nachgewiesen. Es handelt sich um ein standardisiertes Messverfahren. Dies ergibt sich aus der Eichung des Messgerätes durch die zuständige Behörde und der Einhaltung aller Bedingungen einer ordnungsgemäßen Messung. Konkrete Tatsachen, die Zweifel an der Richtigkeit des Messergebnisses begründen könnten, wurden seitens der Verteidigung nicht vorgetragen. Doch wäre auch, wenn man hier nicht von einem standardisierten Messverfahren ausgehen würde, die Ordnungswidrigkeit durch die in die Hauptverhandlung eingeführten Beweismittel nachgewiesen. Im Übrigen ist anzumerken, dass ein solches Sachverständigengutachten, wie es beantragt wurde, nicht geeignet ist, die behauptete Tatsache zu beweisen. Ein Sachverständiger kann lediglich hinsichtlich der technischen Vorgänge bewerten, ob das Messergebnis richtig gewonnen wurde und physikalisch korrekt ist. Dies wurde von der Verteidigung nicht angezweifelt. Fragen der Einhaltung der Bauartzulassung sind Rechtsfragen. Hierzu hat sich ein Sachverständiger nicht zu verhalten, sondern dies ist vom Gericht zu beurteilen. Zweifel an der Einhaltung der Bauartzulassung sind jedoch nach Feststellung des Gerichts in der Hauptverhandlung aufgrund der durchgeführten Beweisaufnahme nicht gegeben. Die Bauartzulassung enthält Vorgaben, die Messgeräte der vorliegenden Art im Allgemeinen erfüllen müssen. Sie ist Grundlage für die Eichung (vgl. §§\\xa03 Nr.\\xa03, 27 MessEG). Auf dem Eichschein vom 03.07.2015 hat das Landesamt für Mess- und Eichwesen Berlin-Brandenburg bestätigt, dass das Gerät geeicht wurde und somit den Anforderungen der Bauartzulassung entspricht. Messdatei, Token und Passwort werden für den Nachweis der Ordnungswidrigkeit nicht benötigt. IV. Diese Ordnungswidrigkeit kann mit einer Geldbuße bis 2.000 € geahndet werden (§\\xa024 Abs.\\xa02 StVG). Da jedoch von Fahrlässigkeit ausgegangen werden muss, da ein Vorsatz nicht nachweisbar ist, reduziert sich dieser Rahmen auf maximal 1.000 € gemäß §\\xa017 Abs.\\xa02 OWiG. Vorliegend wurde die Regelgeldbuße von 80 € gemäß Nr.\\xa011.3.4 Anlage BKatV für handlungs- und schuldangemessen erachtet. Dabei war von einem durchschnittlichen Verschulden und einer durchschnittlichen Schwere der Ordnungswidrigkeit auszugehen. Gründe für ein Abweichen von der Regelgeldbuße lagen nicht vor. V. Die Kostenentscheidung folgt aus §§\\xa046 Abs.\\xa01 OWiG, 465 Abs.\\xa01 StPO.\\n</s>'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2193a1f-4ffe-4f17-a308-fb2bf8361d3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################################################################################################\n",
      "Running training with...\n",
      "105997  training tokens\n",
      "########################################################################################################################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1ee102f4cac41a48c553b24fdef5861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/105997 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "print(\"#\" * 120)\n",
    "print(\"Running training with...\")\n",
    "print(len(dataset), \" training tokens\")\n",
    "print(\"#\" * 120)\n",
    "\n",
    "max_length = 8000\n",
    "output_dir = \"./lorasSFTTrainer\"\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model_with_peft,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 6,\n",
    "        # max_steps = 60, # Set num_train_epochs = 1 for full training runs\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 816,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = output_dir,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ed4827-9aec-499c-8289-95e836b408a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################################################################################################\n",
      "Running training with...\n",
      "105997  training tokens\n",
      "########################################################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 105,997 | Num Epochs = 6\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 79,494\n",
      " \"-____-\"     Number of trainable parameters = 41,943,040\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='79494' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    4/79494 01:38 < 1091:22:06, 0.02 it/s, Epoch 0.00/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "print(\"#\" * 120)\n",
    "print(\"Running training with...\")\n",
    "print(len(dataset), \" training tokens\")\n",
    "print(\"#\" * 120)\n",
    "\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d303752-ebfa-475d-868c-e10ae5006be6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.auto.tokenization_auto because of the following error (look up to see its traceback):\nFailed to import transformers.integrations.ggml because of the following error (look up to see its traceback):\ncannot import name 'Qwen2Converter' from 'transformers.convert_slow_tokenizer' (/home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/utils/import_utils.py:1099\u001b[0m, in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     return False\n\u001b[0;32m-> 1099\u001b[0m # Lastly, check if the `smdistributed` module is present.\n\u001b[1;32m   1100\u001b[0m return _smdistributed_available\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/integrations/ggml.py:28\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AddedToken\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvert_slow_tokenizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT2Converter, LlamaConverter, Qwen2Converter\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Qwen2Converter' from 'transformers.convert_slow_tokenizer' (/home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/utils/import_utils.py:1099\u001b[0m, in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     return False\n\u001b[0;32m-> 1099\u001b[0m # Lastly, check if the `smdistributed` module is present.\n\u001b[1;32m   1100\u001b[0m return _smdistributed_available\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:26\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdynamic_module_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_class_from_dynamic_module, resolve_trust_remote_code\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_gguf_pytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_gguf_checkpoint\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenization_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedTokenizer\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/modeling_gguf_pytorch_utils.py:22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     GGUF_CONFIG_MAPPING,\n\u001b[1;32m     24\u001b[0m     GGUF_TENSOR_MAPPING,\n\u001b[1;32m     25\u001b[0m     GGUF_TOKENIZER_MAPPING,\n\u001b[1;32m     26\u001b[0m     _gguf_parse_value,\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_torch_available\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/utils/import_utils.py:1089\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/utils/import_utils.py:1101\u001b[0m, in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.integrations.ggml because of the following error (look up to see its traceback):\ncannot import name 'Qwen2Converter' from 'transformers.convert_slow_tokenizer' (/home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munsloth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Das ist das gespeicherte Model\u001b[39;00m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/utils/import_utils.py:1090\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1087\u001b[0m except json.JSONDecodeError:\n\u001b[1;32m   1088\u001b[0m     return False\n\u001b[0;32m-> 1090\u001b[0m # Get the sagemaker specific framework parameters from mpi_options variable.\n\u001b[1;32m   1091\u001b[0m mpi_options = os.getenv(\"SM_FRAMEWORK_PARAMS\", \"{}\")\n\u001b[1;32m   1092\u001b[0m try:\n\u001b[1;32m   1093\u001b[0m     # Parse it and check the field \"sagemaker_distributed_dataparallel_enabled\".\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/utils/import_utils.py:1089\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/utils/import_utils.py:1101\u001b[0m, in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.auto.tokenization_auto because of the following error (look up to see its traceback):\nFailed to import transformers.integrations.ggml because of the following error (look up to see its traceback):\ncannot import name 'Qwen2Converter' from 'transformers.convert_slow_tokenizer' (/home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Das ist das gespeicherte Model\n",
    "base_model_id = \"lora_model\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = base_model_id, \n",
    "    max_seq_length = 2048, # Choose any! We auto support RoPE Scaling internally!\n",
    "    dtype = None, # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "    load_in_4bit = True, # Use 4bit quantization to reduce memory usage. Can be False.\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "input_prompt = \" Kurze und einfache Erklärung für die Rechtsprechung und Strafen bei Betrug mit Vorsatz ? \"\n",
    "model_input = tokenizer(input_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=200, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5177775c-af19-470a-b677-44676a377042",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'is_hqq_available' from 'transformers.utils' (/home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/utils/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munsloth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m FastLanguageModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      5\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsloth/mistral-7b-v0.3\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      6\u001b[0m     max_seq_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2048\u001b[39m, \u001b[38;5;66;03m# Choose any! We auto support RoPE Scaling internally!\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;66;03m# None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     load_in_4bit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;66;03m# Use 4bit quantization to reduce memory usage. Can be False.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/unsloth/__init__.py:157\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msave\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_templates\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/unsloth/models/__init__.py:15\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloader\u001b[39;00m  \u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllama\u001b[39;00m   \u001b[38;5;28;01mimport\u001b[39;00m FastLlamaModel\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmistral\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastMistralModel\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/unsloth/models/loader.py:15\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_bfloat16_supported, HAS_FLASH_ATTENTION, HAS_FLASH_ATTENTION_SOFTCAPPING\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllama\u001b[39;00m   \u001b[38;5;28;01mimport\u001b[39;00m FastLlamaModel, logger\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmistral\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastMistralModel\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/unsloth/models/_utils.py:144\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# =============================================\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# =============================================\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# Fix KeyError: 'Cache only has 0 layers, attempted to access layer with index 0'\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcache_utils\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformers\u001b[38;5;241m.\u001b[39mcache_utils, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDynamicCache\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    146\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mcache_utils\u001b[38;5;241m.\u001b[39mDynamicCache\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__cache_utils_getitem__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    148\u001b[0m     source \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetsource(transformers\u001b[38;5;241m.\u001b[39mcache_utils\u001b[38;5;241m.\u001b[39mDynamicCache\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/cache_utils.py:12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PretrainedConfig\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_hqq_available, is_quanto_available, is_torchdynamo_compiling, logging\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_quanto_available():\n\u001b[1;32m     16\u001b[0m     quanto_version \u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquanto\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'is_hqq_available' from 'transformers.utils' (/home/ps2024/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/utils/__init__.py)"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/mistral-7b-v0.3\", \n",
    "    max_seq_length = 2048, # Choose any! We auto support RoPE Scaling internally!\n",
    "    dtype = None, # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "    load_in_4bit = True, # Use 4bit quantization to reduce memory usage. Can be False.\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "input_prompt = \" Kurze und einfache Erklärung für die Rechtsprechung und Strafen bei Betrug mit Vorsatz ? \"\n",
    "model_input = tokenizer(input_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=200, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936cbff3-8994-4d0d-8854-2c35938b4f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
