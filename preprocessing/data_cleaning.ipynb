{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31a86293-1d95-4b1d-96fa-decb2612d7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackabuse.com/reading-and-writing-xml-files-in-python-with-pandas/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e299bdb-ad37-421b-ae61-4e5f5c2b4117",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f23e7729-72a3-4564-9ce8-23560f895ec2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lxml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01municodedata\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlxml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m etree\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lxml'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import unicodedata\n",
    "from lxml import etree\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Directory paths\n",
    "input_dir = './bgh_urteile/'\n",
    "output_dir = './normalized_bgh_urteile/'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea57291-708b-4ecf-9571-4eb2b5cb7cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize text using unicodedata\n",
    "def normalize_text(text):\n",
    "    return unicodedata.normalize('NFKC', text)\n",
    "\n",
    "# Function to create 10-grams from text\n",
    "def create_ngrams(text, n=10):\n",
    "    words = text.split()\n",
    "    ngrams = [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "    return ngrams\n",
    "\n",
    "# Function to extract all text content from an XML element\n",
    "def extract_text_and_elements(root):\n",
    "    texts = []\n",
    "    elements = []\n",
    "    for elem in root.iter():\n",
    "        if elem.text:\n",
    "            texts.append(elem.text.strip())\n",
    "            elements.append(elem)\n",
    "    return texts, elements\n",
    "\n",
    "# Function to remove duplicated 10-grams from text\n",
    "def remove_duplicated_ngrams(text, duplicated_ngrams, ngram_tracker):\n",
    "    deleted_ngrams = []\n",
    "    for ngram in duplicated_ngrams:\n",
    "        count = ngram_tracker[ngram]\n",
    "        occurrences = text.count(ngram)\n",
    "        if occurrences > 1:\n",
    "            text = text.replace(ngram, '', occurrences - 1)\n",
    "            deleted_ngrams.append(ngram)\n",
    "    return text, deleted_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1bd653-0b55-47b6-b076-ebef56a840d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(input_dir):\n",
    "    for filename in files:\n",
    "        if filename.endswith('.xml') and 'checkpoint' not in filename:\n",
    "            input_path = os.path.join(root, filename)\n",
    "            normalized_filename = f\"normalized_{filename}\"\n",
    "            relpath = os.path.relpath(root, input_dir)\n",
    "            output_subdir = os.path.join(output_dir, relpath)\n",
    "            os.makedirs(output_subdir, exist_ok=True)\n",
    "            output_path = os.path.join(output_subdir, normalized_filename)\n",
    "            \n",
    "            # Parse the XML file\n",
    "            tree = etree.parse(input_path)\n",
    "            root_element = tree.getroot()\n",
    "\n",
    "            # Normalize text in XML elements\n",
    "            for element in root_element.iter():\n",
    "                if element.text:\n",
    "                    element.text = normalize_text(element.text)\n",
    "\n",
    "            # Extract normalized text and elements\n",
    "            normalized_texts, normalized_elements = extract_text_and_elements(root_element)\n",
    "            \n",
    "            # Create 10-grams from normalized text\n",
    "            all_ngrams = []\n",
    "            for text in normalized_texts:\n",
    "                all_ngrams.extend(create_ngrams(text))\n",
    "\n",
    "            # Count the 10-grams to find duplicates\n",
    "            ngram_counts = Counter(all_ngrams)\n",
    "            \n",
    "            # Find duplicated 10-grams\n",
    "            duplicated_ngrams = [ngram for ngram, count in ngram_counts.items() if count > 1]\n",
    "\n",
    "            # Track deleted n-grams\n",
    "            deleted_ngrams = []\n",
    "\n",
    "            # Initialize n-gram tracker to track deletions\n",
    "            ngram_tracker = ngram_counts.copy()\n",
    "\n",
    "            # Remove duplicated 10-grams from normalized text elements\n",
    "            for element in normalized_elements:\n",
    "                if element.text:\n",
    "                    element.text, deleted = remove_duplicated_ngrams(element.text, duplicated_ngrams, ngram_tracker)\n",
    "                    deleted_ngrams.extend(deleted)\n",
    "\n",
    "            # Save the modified XML file\n",
    "            tree.write(output_path, pretty_print=True, xml_declaration=True, encoding='UTF-8')\n",
    "\n",
    "            # Print deleted n-grams\n",
    "            if deleted_ngrams:\n",
    "                print(f\"Deleted n-grams in file {filename}:\")\n",
    "                for ngram in deleted_ngrams:\n",
    "                    print(ngram)\n",
    "\n",
    "print(\"Normalization and duplicate removal completed for all files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57e380b-6d1f-4078-93b7-a80019847574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Display the 10-grams\n",
    "df_ngrams = pd.DataFrame(all_ngrams, columns=['10-Gram'])\n",
    "# Set display options to show all columns and rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "display(df_ngrams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
